{"componentChunkName":"component---src-templates-post-tsx-content-file-path-src-posts-2019-nano-neuron-index-md","path":"/blog/2019/nano-neuron/","result":{"data":{"mdx":{"id":"0e486034-b509-5cc1-a451-0af4142ddfe9","body":"\r\n## TL;DR\r\n\r\n[NanoNeuron](https://github.com/trekhleb/nano-neuron) is an _over-simplified_ version of the Neuron concept from Neural Networks. NanoNeuron is trained to convert temperature values from Celsius to Fahrenheit.\r\n\r\nThe [NanoNeuron.js](https://github.com/trekhleb/nano-neuron/blob/master/NanoNeuron.js) code example contains 7 simple JavaScript functions (which touches on model prediction, cost calculation, forward/backwards propagation, and training) that will give you a feeling of how machines can actually \"learn\". No 3rd-party libraries, no external data-sets or dependencies, only pure and simple JavaScript functions.\r\n\r\nâ˜ðŸ»These functions are **NOT**, by any means, a complete guide to machine learning. A lot of machine learning concepts are skipped and over-simplified! This simplification is done on purpose to give the reader a really **basic** understanding and feeling of how machines can learn and ultimately to make it possible for the reader to recognize that it's not \"machine learning MAGIC\" but rather \"machine learning MATH\" ðŸ¤“.\r\n\r\n## What our NanoNeuron will learn\r\n\r\nYou've probably heard about Neurons in the context of [Neural Networks](https://en.wikipedia.org/wiki/Neural_network). NanoNeuron is just that but simpler and we're going to implement it from scratch. For simplicity reasons we're not even going to build a network on NanoNeurons. We will have it all working on its own, doing some magical predictions for us. Namely, we will teach this singular NanoNeuron to convert (predict) the temperature from Celsius to Fahrenheit.\r\n\r\nBy the way, the formula for converting Celsius to Fahrenheit is this:\r\n\r\n![Celsius to Fahrenheit](assets/02-formula-C-to-F.png)\r\n\r\nBut for now our NanoNeuron doesn't know about it...\r\n\r\n### The NanoNeuron model\r\n\r\nLet's implement our NanoNeuron model function. It implements basic linear dependency between `x` and `y` which looks like `y = w * x + b`. Simply saying our NanoNeuron is a \"kid\" in a \"school\" that is being taught to draw the straight line in `XY` coordinates.\r\n\r\nVariables `w`, `b` are parameters of the model. NanoNeuron knows only about these two parameters of the linear function.\r\nThese parameters are something that NanoNeuron is going to \"learn\" during the training process.\r\n\r\nThe only thing that NanoNeuron can do is to imitate linear dependency. In its `predict()` method it accepts some input `x` and predicts the output `y`. No magic here.\r\n\r\n```javascript\r\nfunction NanoNeuron(w, b) {\r\n  this.w = w;\r\n  this.b = b;\r\n  this.predict = (x) => {\r\n    return x * this.w + this.b;\r\n  }\r\n}\r\n```\r\n\r\n...wait... [linear regression](https://en.wikipedia.org/wiki/Linear_regression) is it you?  ðŸ§\r\n\r\n### Celsius to Fahrenheit conversion\r\n\r\nThe temperature value in Celsius can be converted to Fahrenheit using the following formula: `f = 1.8 * c + 32`, where `c` is a temperature in Celsius and `f` is the calculated temperature in Fahrenheit.\r\n\r\n```javascript\r\nfunction celsiusToFahrenheit(c) {\r\n  const w = 1.8;\r\n  const b = 32;\r\n  const f = c * w + b;\r\n  return f;\r\n};\r\n```\r\n\r\nUltimately we want to teach our NanoNeuron to imitate this function (to learn that `w = 1.8` and `b = 32`) without knowing these parameters in advance.\r\n\r\nThis is how the Celsius to Fahrenheit conversion function looks like:\r\n\r\n![Celsius to Fahrenheit conversion](assets/05.png)\r\n\r\n### Generating data-sets\r\n\r\nBefore the training we need to generate **training** and **test data-sets** based on the `celsiusToFahrenheit()` function. Data-sets consist of pairs of input values and correctly labeled output values.\r\n\r\n> In real life, in most of cases, this data would be collected rather than generated. For example, we might have a set of images of hand-drawn numbers and the corresponding set of numbers that explains what number is written on each picture.\r\n\r\nWe will use TRAINING example data to train our NanoNeuron. Before our NanoNeuron will grow and be able to make decisions on its own, we need to teach it what is right and what is wrong using training examples.\r\n\r\nWe will use TEST examples to evaluate how well our NanoNeuron performs on the data that it didn't see during the training. This is the point where we could see that our \"kid\" has grown and can make decisions on its own.\r\n\r\n```javascript\r\nfunction generateDataSets() {\r\n  // xTrain -> [0, 1, 2, ...],\r\n  // yTrain -> [32, 33.8, 35.6, ...]\r\n  const xTrain = [];\r\n  const yTrain = [];\r\n  for (let x = 0; x < 100; x += 1) {\r\n    const y = celsiusToFahrenheit(x);\r\n    xTrain.push(x);\r\n    yTrain.push(y);\r\n  }\r\n\r\n  // xTest -> [0.5, 1.5, 2.5, ...]\r\n  // yTest -> [32.9, 34.7, 36.5, ...]\r\n  const xTest = [];\r\n  const yTest = [];\r\n  // By starting from 0.5 and using the same step of 1 as we have used for training set\r\n  // we make sure that test set has different data comparing to training set.\r\n  for (let x = 0.5; x < 100; x += 1) {\r\n    const y = celsiusToFahrenheit(x);\r\n    xTest.push(x);\r\n    yTest.push(y);\r\n  }\r\n\r\n  return [xTrain, yTrain, xTest, yTest];\r\n}\r\n```\r\n\r\n### The cost (the error) of prediction\r\n\r\nWe need to have some metric that will show us how close our model's prediction is to correct values. The calculation of the cost (the mistake) between the correct output value of `y` and `prediction`, that our NanoNeuron created, will be made using the following formula:\r\n\r\n![Prediction Cost](assets/06.png)\r\n\r\nThis is a simple difference between two values. The closer the values are to each other, the smaller the difference. We're using a power of `2` here just to get rid of negative numbers so that `(1 - 2) ^ 2` would be the same as `(2 - 1) ^ 2`. Division by `2` is happening just to simplify further the backward propagation formula (see below).\r\n\r\nThe cost function in this case will be as simple as:\r\n\r\n```javascript\r\nfunction predictionCost(y, prediction) {\r\n  return (y - prediction) ** 2 / 2; // i.e. -> 235.6\r\n}\r\n```\r\n\r\n### Forward propagation\r\n\r\nTo do forward propagation means to do a prediction for all training examples from `xTrain` and `yTrain` data-sets and to calculate the average cost of those predictions along the way.\r\n\r\nWe just let our NanoNeuron say its opinion, at this point, by just allowing it to guess how to convert the temperature. It might be stupidly wrong here. The average cost will show us how wrong our model is right now. This cost value is really important since changing the NanoNeuron parameters `w` and `b`, and by doing the forward propagation again; we will be able to evaluate if our NanoNeuron became smarter or not after these parameters change.\r\n\r\nThe average cost will be calculated using the following formula:\r\n\r\n![Average Cost](assets/07.png)\r\n\r\nWhere `m` is a number of training examples (in our case: `100`).\r\n\r\nHere is how we may implement it in code:\r\n\r\n```javascript\r\nfunction forwardPropagation(model, xTrain, yTrain) {\r\n  const m = xTrain.length;\r\n  const predictions = [];\r\n  let cost = 0;\r\n  for (let i = 0; i < m; i += 1) {\r\n    const prediction = nanoNeuron.predict(xTrain[i]);\r\n    cost += predictionCost(yTrain[i], prediction);\r\n    predictions.push(prediction);\r\n  }\r\n  // We are interested in average cost.\r\n  cost /= m;\r\n  return [predictions, cost];\r\n}\r\n```\r\n\r\n### Backward propagation\r\n\r\nWhen we know how right or wrong our NanoNeuron's predictions are (based on average cost at this point) what should we do to make the predictions more precise?\r\n\r\nThe backward propagation gives us the answer to this question. Backward propagation is the process of evaluating the cost of prediction and adjusting the NanoNeuron's parameters `w` and `b` so that next and future predictions would be more precise.\r\n\r\nThis is the place where machine learning looks like magic ðŸ§žâ€â™‚ï¸. The key concept here is the **derivative** which shows what step to take to get closer to the cost function minimum.\r\n\r\nRemember, finding the minimum of a cost function is the ultimate goal of the training process. If we find such values for `w` and `b` such that our average cost function will be small, it would mean that the NanoNeuron model does really good and precise predictions.\r\n\r\nDerivatives are a big and separate topic that we will not cover in this article. [MathIsFun](https://www.mathsisfun.com/calculus/derivatives-introduction.html) is a good resource to get a basic understanding of it.\r\n\r\nOne thing about derivatives that will help you to understand how backward propagation works is that the derivative, by its meaning, is a tangent line to the function curve that points toward the direction of the function minimum.\r\n\r\n![Derivative slope](assets/2.svg)\r\n\r\n_Image source: [MathIsFun](https://www.mathsisfun.com/calculus/derivatives-introduction.html)_\r\n\r\nFor example, on the plot above, you can see that if we're at the point of `(x=2, y=4)` then the slope tells us to go `left` and `down` to get to the function minimum. Also notice that the bigger the slope, the faster we should move to the minimum.\r\n\r\nThe derivatives of our `averageCost` function for parameters `w` and `b` looks like this:\r\n\r\n![dW](assets/08.png)\r\n\r\n![dB](assets/09.png)\r\n\r\nWhere `m` is a number of training examples (in our case: `100`).\r\n\r\n_You may read more about derivative rules and how to get a derivative of complex functions [here](https://www.mathsisfun.com/calculus/derivatives-rules.html)._\r\n\r\n```javascript\r\nfunction backwardPropagation(predictions, xTrain, yTrain) {\r\n  const m = xTrain.length;\r\n  // At the beginning we don't know in which way our parameters 'w' and 'b' need to be changed.\r\n  // Therefore we're setting up the changing steps for each parameters to 0.\r\n  let dW = 0;\r\n  let dB = 0;\r\n  for (let i = 0; i < m; i += 1) {\r\n    dW += (yTrain[i] - predictions[i]) * xTrain[i];\r\n    dB += yTrain[i] - predictions[i];\r\n  }\r\n  // We're interested in average deltas for each params.\r\n  dW /= m;\r\n  dB /= m;\r\n  return [dW, dB];\r\n}\r\n```\r\n\r\n### Training the model\r\n\r\nNow we know how to evaluate the correctness of our model for all training set examples (_forward propagation_). We also know how to do small adjustments to parameters `w` and `b` of our NanoNeuron model (_backward propagation_). But the issue is that if we run forward propagation and then backward propagation only once, it won't be enough for our model to learn any laws/trends from the training data. You may compare it with attending a one day of elementary school for the kid. He/she should go to the school not once but day after day and year after year to learn something.\r\n\r\nSo we need to repeat forward and backward propagation for our model many times. That is exactly what the `trainModel()` function does. It is like a \"teacher\" for our NanoNeuron model:\r\n\r\n- it will spend some time (`epochs`) with our slightly stupid NanoNeuron model and try to train/teach it,\r\n- it will use specific \"books\" (`xTrain` and `yTrain` data-sets) for training,\r\n- it will push our kid to learn harder (faster) by using a learning rate parameter `alpha`\r\n\r\nA few words about the learning rate `alpha`. This is just a multiplier for `dW` and `dB` values we have calculated during the backward propagation. So, derivative pointed us toward the direction we need to take to find a minimum of the cost function (`dW` and `dB` sign) and it also showed us how fast we need to go in that direction (absolute values of `dW` and `dB`). Now we need to multiply those step sizes to `alpha` just to adjust our movement to the minimum faster or slower. Sometimes if we use big values for `alpha`, we might simply jump over the minimum and never find it.\r\n\r\nThe analogy with the teacher would be that the harder s/he pushes our \"nano-kid\" the faster our \"nano-kid\" will learn but if the teacher pushes too hard, the \"kid\" will have a nervous breakdown and won't be able to learn anything ðŸ¤¯.\r\n\r\nHere is how we're going to update our model's `w` and `b` params:\r\n\r\n![w](assets/10.png)\r\n\r\n![b](assets/11.png)\r\n\r\nHere is our trainer function:\r\n\r\n```javascript\r\nfunction trainModel({model, epochs, alpha, xTrain, yTrain}) {\r\n  // The is the history array of how NanoNeuron learns.\r\n  const costHistory = [];\r\n\r\n  // Let's start counting epochs.\r\n  for (let epoch = 0; epoch < epochs; epoch += 1) {\r\n    // Forward propagation.\r\n    const [predictions, cost] = forwardPropagation(model, xTrain, yTrain);\r\n    costHistory.push(cost);\r\n\r\n    // Backward propagation.\r\n    const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain);\r\n\r\n    // Adjust our NanoNeuron parameters to increase accuracy of our model predictions.\r\n    nanoNeuron.w += alpha * dW;\r\n    nanoNeuron.b += alpha * dB;\r\n  }\r\n\r\n  return costHistory;\r\n}\r\n```\r\n\r\n### Putting all the pieces together\r\n\r\nNow let's use the functions we have created above.\r\n\r\nLet's create our NanoNeuron model instance. At this moment the NanoNeuron doesn't know what values should be set for parameters `w` and `b`. So let's set up `w` and `b` randomly.\r\n\r\n```javascript\r\nconst w = Math.random(); // i.e. -> 0.9492\r\nconst b = Math.random(); // i.e. -> 0.4570\r\nconst nanoNeuron = new NanoNeuron(w, b);\r\n```\r\n\r\nGenerate training and test data-sets.\r\n\r\n```javascript\r\nconst [xTrain, yTrain, xTest, yTest] = generateDataSets();\r\n```\r\n\r\nLet's train the model with small incremental (`0.0005`) steps for `70000` epochs. You can play with these parameters, they are being defined empirically.\r\n\r\n```javascript\r\nconst epochs = 70000;\r\nconst alpha = 0.0005;\r\nconst trainingCostHistory = trainModel({model: nanoNeuron, epochs, alpha, xTrain, yTrain});\r\n```\r\n\r\nLet's check how the cost function was changing during the training. We're expecting that the cost after the training should be much lower than before. This would mean that NanoNeuron got smarter. The opposite is also possible.\r\n\r\n```javascript\r\nconsole.log('Cost before the training:', trainingCostHistory[0]); // i.e. -> 4694.3335043\r\nconsole.log('Cost after the training:', trainingCostHistory[epochs - 1]); // i.e. -> 0.0000024\r\n```\r\n\r\nThis is how the training cost changes over the epochs. On the `x` axes is the epoch number x1000.\r\n\r\n![Training process](assets/12.png)\r\n\r\nLet's take a look at NanoNeuron parameters to see what it has learned. We expect that NanoNeuron parameters `w` and `b` to be similar to ones we have in the `celsiusToFahrenheit()` function (`w = 1.8` and `b = 32`) since our NanoNeuron tried to imitate it.\r\n\r\n```javascript\r\nconsole.log('NanoNeuron parameters:', {w: nanoNeuron.w, b: nanoNeuron.b}); // i.e. -> {w: 1.8, b: 31.99}\r\n```\r\n\r\nEvaluate the model accuracy for the test data-set to see how well our NanoNeuron deals with new unknown data predictions. The cost of predictions on test sets is expected to be close to the training cost. This would mean that our NanoNeuron performs well on known and unknown data.\r\n\r\n```javascript\r\n[testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest);\r\nconsole.log('Cost on new testing data:', testCost); // i.e. -> 0.0000023\r\n```\r\n\r\nNow, since we see that our NanoNeuron \"kid\" has performed well in the \"school\" during the training and that he can convert Celsius to Fahrenheit temperatures correctly, even for the data it hasn't seen, we can call it \"smart\" and ask him some questions. This was the ultimate goal of the entire training process.\r\n\r\n```javascript\r\nconst tempInCelsius = 70;\r\nconst customPrediction = nanoNeuron.predict(tempInCelsius);\r\nconsole.log(`NanoNeuron \"thinks\" that ${tempInCelsius}Â°C in Fahrenheit is:`, customPrediction); // -> 158.0002\r\nconsole.log('Correct answer is:', celsiusToFahrenheit(tempInCelsius)); // -> 158\r\n```\r\n\r\nSo close! As all of us humans, our NanoNeuron is good but not ideal :)\r\n\r\nHappy learning to you!\r\n\r\n## How to launch NanoNeuron\r\n\r\nYou may clone the repository and run it locally:\r\n\r\n```bash\r\ngit clone https://github.com/trekhleb/nano-neuron.git\r\ncd nano-neuron\r\n```\r\n\r\n```bash\r\nnode ./NanoNeuron.js\r\n```\r\n\r\n## Skipped machine learning concepts\r\n\r\nThe following machine learning concepts were skipped and simplified for simplicity of explanation.\r\n\r\n**Training/testing data-set splitting**\r\n\r\nNormally you have one big set of data. Depending on the number of examples in that set, you may want to split it in proportion of 70/30 for train/test sets. The data in the set should be randomly shuffled before the split. If the number of examples is big (i.e. millions) then the split might happen in proportions that are closer to 90/10 or 95/5 for train/test data-sets.\r\n\r\n**The network brings the power**\r\n\r\nNormally you won't notice the usage of just one standalone neuron. The power is in the [network](https://en.wikipedia.org/wiki/Neural_network) of such neurons. The network might learn much more complex features. NanoNeuron alone looks more like a simple [linear regression](https://en.wikipedia.org/wiki/Linear_regression) than a neural network.\r\n\r\n**Input normalization**\r\n\r\nBefore the training, it would be better to [normalize input values](https://www.jeremyjordan.me/batch-normalization/).\r\n\r\n**Vectorized implementation**\r\n\r\nFor networks, the vectorized (matrix) calculations work much faster than `for` loops. Normally forward/backward propagation works much faster if it is implemented in vectorized form and calculated using, for example, [Numpy](https://numpy.org/) Python library.\r\n\r\n**Minimum of the cost function**\r\n\r\nThe cost function that we were using in this example is over-simplified. It should have [logarithmic components](https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression/32998675). Changing the cost function will also change its derivatives so the back propagation step would also use different formulas.\r\n\r\n**Activation function**\r\n\r\nNormally the output of a neuron should be passed through an activation function like [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) or [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) or others.\r\n","fields":{"slug":"/blog/2019/nano-neuron/"},"internal":{"contentFilePath":"C:/prj/quangphucphung.github.io/src/posts/2019/nano-neuron/index.md"},"frontmatter":{"title":"NanoNeuron - 7 simple JS functions that explain how machines learn","summary":"The NanoNeuron.js code example contains 7 simple JavaScript functions (which touches on model prediction, cost calculation, forward/backwards propagation, and training) that will give you a feeling of how machines can actually learn. No 3rd-party libraries, no external data-sets or dependencies, only pure and simple JavaScript functions.","date":"06 December, 2019","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8c920df1600d388655fe4289a999c280/fb46f/01-cover.png","srcSet":"/static/8c920df1600d388655fe4289a999c280/fb46f/01-cover.png 720w","sizes":"100vw"},"sources":[{"srcSet":"/static/8c920df1600d388655fe4289a999c280/8c4bd/01-cover.webp 720w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.7527777777777778}}}}}},"pageContext":{"slug":"/blog/2019/nano-neuron/","frontmatter":{"title":"NanoNeuron - 7 simple JS functions that explain how machines learn","summary":"The NanoNeuron.js code example contains 7 simple JavaScript functions (which touches on model prediction, cost calculation, forward/backwards propagation, and training) that will give you a feeling of how machines can actually learn. No 3rd-party libraries, no external data-sets or dependencies, only pure and simple JavaScript functions.","cover":"assets/01-cover.png","date":"2019-12-06T00:00:00.000Z"}}},"staticQueryHashes":["3196427994"],"slicesMap":{}}